{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('output')\n",
    "model_name = 'cascade_lr'\n",
    "model_dir = output_dir/model_name\n",
    "config_path = model_dir/'config.yaml'\n",
    "weights_path = model_dir/'model_0007999.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): CascadeROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): ModuleList(\n      (0): FastRCNNConvFCHead(\n        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      )\n      (1): FastRCNNConvFCHead(\n        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      )\n      (2): FastRCNNConvFCHead(\n        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      )\n    )\n    (box_predictor): ModuleList(\n      (0): FastRCNNOutputLayers(\n        (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n      )\n      (1): FastRCNNOutputLayers(\n        (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n      )\n      (2): FastRCNNOutputLayers(\n        (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n      )\n    )\n  )\n)\n\u001b[32m[06/13 11:09:00 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from output/cascade_lr/model_0007999.pth\n\u001b[32m[06/13 11:09:00 d2.data.datasets.coco]: \u001b[0mLoaded 1474 images in COCO format from assets/val_clean.json\n\u001b[32m[06/13 11:09:00 d2.data.build]: \u001b[0mDistribution of instances among all 5 categories:\n\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n|    tops    | 317          |  trousers  | 313          | outerwear  | 316          |\n|  dresses   | 1338         |   skirts   | 174          |            |              |\n|   total    | 2458         |            |              |            |              |\u001b[0m\n\u001b[32m[06/13 11:09:00 d2.data.common]: \u001b[0mSerializing 1474 elements to byte tensors and concatenating them all ...\n\u001b[32m[06/13 11:09:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.37 MiB\n\u001b[32m[06/13 11:09:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 1474 images\n\u001b[32m[06/13 11:09:11 d2.evaluation.evaluator]: \u001b[0mInference done 2/1474. 5.2163 s / img. ETA=2:12:25\n\u001b[32m[06/13 11:09:18 d2.evaluation.evaluator]: \u001b[0mInference done 3/1474. 5.6357 s / img. ETA=2:21:08\n\u001b[32m[06/13 11:09:24 d2.evaluation.evaluator]: \u001b[0mInference done 4/1474. 5.7622 s / img. ETA=2:23:24\n\u001b[32m[06/13 11:09:30 d2.evaluation.evaluator]: \u001b[0mInference done 5/1474. 5.9305 s / img. ETA=2:26:59\n\u001b[32m[06/13 11:09:37 d2.evaluation.evaluator]: \u001b[0mInference done 6/1474. 6.3131 s / img. ETA=2:34:28\n\u001b[32m[06/13 11:09:42 d2.evaluation.evaluator]: \u001b[0mInference done 7/1474. 5.9092 s / img. ETA=2:24:29\n\u001b[32m[06/13 11:09:53 d2.evaluation.evaluator]: \u001b[0mInference done 9/1474. 5.6734 s / img. ETA=2:18:32\n\u001b[32m[06/13 11:09:59 d2.evaluation.evaluator]: \u001b[0mInference done 10/1474. 5.7036 s / img. ETA=2:19:11\n\u001b[32m[06/13 11:10:05 d2.evaluation.evaluator]: \u001b[0mInference done 11/1474. 5.7119 s / img. ETA=2:19:18\n\u001b[32m[06/13 11:10:18 d2.evaluation.evaluator]: \u001b[0mInference done 13/1474. 5.8796 s / img. ETA=2:23:11\n\u001b[32m[06/13 11:10:24 d2.evaluation.evaluator]: \u001b[0mInference done 14/1474. 5.9271 s / img. ETA=2:24:15\n\u001b[32m[06/13 11:10:31 d2.evaluation.evaluator]: \u001b[0mInference done 15/1474. 6.0405 s / img. ETA=2:26:54\n\u001b[32m[06/13 11:10:37 d2.evaluation.evaluator]: \u001b[0mInference done 16/1474. 6.0420 s / img. ETA=2:26:50\n\u001b[32m[06/13 11:10:43 d2.evaluation.evaluator]: \u001b[0mInference done 17/1474. 6.0338 s / img. ETA=2:26:33\n\u001b[32m[06/13 11:10:48 d2.evaluation.evaluator]: \u001b[0mInference done 18/1474. 5.9898 s / img. ETA=2:25:23\n\u001b[32m[06/13 11:10:54 d2.evaluation.evaluator]: \u001b[0mInference done 19/1474. 5.9436 s / img. ETA=2:24:09\n\u001b[32m[06/13 11:11:00 d2.evaluation.evaluator]: \u001b[0mInference done 20/1474. 5.9731 s / img. ETA=2:24:46\n\u001b[32m[06/13 11:11:06 d2.evaluation.evaluator]: \u001b[0mInference done 21/1474. 5.9664 s / img. ETA=2:24:31\n\u001b[32m[06/13 11:11:12 d2.evaluation.evaluator]: \u001b[0mInference done 22/1474. 5.9670 s / img. ETA=2:24:26\n\u001b[32m[06/13 11:11:19 d2.evaluation.evaluator]: \u001b[0mInference done 23/1474. 6.0336 s / img. ETA=2:25:56\n\u001b[32m[06/13 11:11:26 d2.evaluation.evaluator]: \u001b[0mInference done 24/1474. 6.0602 s / img. ETA=2:26:29\n\u001b[32m[06/13 11:11:32 d2.evaluation.evaluator]: \u001b[0mInference done 25/1474. 6.0882 s / img. ETA=2:27:03\n\u001b[32m[06/13 11:11:38 d2.evaluation.evaluator]: \u001b[0mInference done 26/1474. 6.0901 s / img. ETA=2:27:00\n\u001b[32m[06/13 11:11:45 d2.evaluation.evaluator]: \u001b[0mInference done 27/1474. 6.1072 s / img. ETA=2:27:19\n\u001b[32m[06/13 11:11:51 d2.evaluation.evaluator]: \u001b[0mInference done 28/1474. 6.0874 s / img. ETA=2:26:44\n\u001b[32m[06/13 11:11:57 d2.evaluation.evaluator]: \u001b[0mInference done 29/1474. 6.0879 s / img. ETA=2:26:38\n\u001b[32m[06/13 11:12:03 d2.evaluation.evaluator]: \u001b[0mInference done 30/1474. 6.0844 s / img. ETA=2:26:27\n\u001b[32m[06/13 11:12:09 d2.evaluation.evaluator]: \u001b[0mInference done 31/1474. 6.1065 s / img. ETA=2:26:53\n\u001b[32m[06/13 11:12:16 d2.evaluation.evaluator]: \u001b[0mInference done 32/1474. 6.1284 s / img. ETA=2:27:19\n\u001b[32m[06/13 11:12:22 d2.evaluation.evaluator]: \u001b[0mInference done 33/1474. 6.1375 s / img. ETA=2:27:26\n\u001b[32m[06/13 11:12:28 d2.evaluation.evaluator]: \u001b[0mInference done 34/1474. 6.1311 s / img. ETA=2:27:10\n\u001b[32m[06/13 11:12:36 d2.evaluation.evaluator]: \u001b[0mInference done 35/1474. 6.1782 s / img. ETA=2:28:12\n\u001b[32m[06/13 11:12:42 d2.evaluation.evaluator]: \u001b[0mInference done 36/1474. 6.1917 s / img. ETA=2:28:25\n\u001b[32m[06/13 11:12:48 d2.evaluation.evaluator]: \u001b[0mInference done 37/1474. 6.1672 s / img. ETA=2:27:44\n\u001b[32m[06/13 11:12:53 d2.evaluation.evaluator]: \u001b[0mInference done 38/1474. 6.1333 s / img. ETA=2:26:49\n\u001b[32m[06/13 11:12:59 d2.evaluation.evaluator]: \u001b[0mInference done 39/1474. 6.1249 s / img. ETA=2:26:31\n\u001b[32m[06/13 11:13:05 d2.evaluation.evaluator]: \u001b[0mInference done 40/1474. 6.1169 s / img. ETA=2:26:13\n\u001b[32m[06/13 11:13:12 d2.evaluation.evaluator]: \u001b[0mInference done 41/1474. 6.1497 s / img. ETA=2:26:54\n\u001b[32m[06/13 11:13:21 d2.evaluation.evaluator]: \u001b[0mInference done 42/1474. 6.2168 s / img. ETA=2:28:24\n\u001b[32m[06/13 11:13:26 d2.evaluation.evaluator]: \u001b[0mInference done 43/1474. 6.1905 s / img. ETA=2:27:40\n\u001b[32m[06/13 11:13:33 d2.evaluation.evaluator]: \u001b[0mInference done 44/1474. 6.2061 s / img. ETA=2:27:56\n\u001b[32m[06/13 11:13:39 d2.evaluation.evaluator]: \u001b[0mInference done 45/1474. 6.2034 s / img. ETA=2:27:46\n\u001b[32m[06/13 11:13:46 d2.evaluation.evaluator]: \u001b[0mInference done 46/1474. 6.2214 s / img. ETA=2:28:06\n\u001b[32m[06/13 11:13:52 d2.evaluation.evaluator]: \u001b[0mInference done 47/1474. 6.2260 s / img. ETA=2:28:06\n\u001b[32m[06/13 11:13:59 d2.evaluation.evaluator]: \u001b[0mInference done 48/1474. 6.2541 s / img. ETA=2:28:40\n\u001b[32m[06/13 11:14:06 d2.evaluation.evaluator]: \u001b[0mInference done 49/1474. 6.2708 s / img. ETA=2:28:57\n\u001b[32m[06/13 11:14:13 d2.evaluation.evaluator]: \u001b[0mInference done 50/1474. 6.2721 s / img. ETA=2:28:53\n\u001b[32m[06/13 11:14:24 d2.evaluation.evaluator]: \u001b[0mInference done 52/1474. 6.2390 s / img. ETA=2:27:53\n\u001b[32m[06/13 11:14:33 d2.evaluation.evaluator]: \u001b[0mInference done 53/1474. 6.2955 s / img. ETA=2:29:07\n\u001b[32m[06/13 11:14:40 d2.evaluation.evaluator]: \u001b[0mInference done 54/1474. 6.3053 s / img. ETA=2:29:15\n\u001b[32m[06/13 11:14:46 d2.evaluation.evaluator]: \u001b[0mInference done 55/1474. 6.3110 s / img. ETA=2:29:17\n\u001b[32m[06/13 11:14:54 d2.evaluation.evaluator]: \u001b[0mInference done 56/1474. 6.3429 s / img. ETA=2:29:56\n\u001b[32m[06/13 11:15:01 d2.evaluation.evaluator]: \u001b[0mInference done 57/1474. 6.3561 s / img. ETA=2:30:08\n\u001b[32m[06/13 11:15:09 d2.evaluation.evaluator]: \u001b[0mInference done 58/1474. 6.3793 s / img. ETA=2:30:35\n\u001b[32m[06/13 11:15:16 d2.evaluation.evaluator]: \u001b[0mInference done 59/1474. 6.3894 s / img. ETA=2:30:42\n\u001b[32m[06/13 11:15:23 d2.evaluation.evaluator]: \u001b[0mInference done 60/1474. 6.4025 s / img. ETA=2:30:55\n\u001b[32m[06/13 11:15:29 d2.evaluation.evaluator]: \u001b[0mInference done 61/1474. 6.4005 s / img. ETA=2:30:45\n\u001b[32m[06/13 11:15:36 d2.evaluation.evaluator]: \u001b[0mInference done 62/1474. 6.4073 s / img. ETA=2:30:49\n\u001b[32m[06/13 11:15:43 d2.evaluation.evaluator]: \u001b[0mInference done 63/1474. 6.4238 s / img. ETA=2:31:05\n\u001b[32m[06/13 11:15:55 d2.evaluation.evaluator]: \u001b[0mInference done 65/1474. 6.4015 s / img. ETA=2:30:21\n\u001b[32m[06/13 11:16:01 d2.evaluation.evaluator]: \u001b[0mInference done 66/1474. 6.3972 s / img. ETA=2:30:09\n\u001b[32m[06/13 11:16:08 d2.evaluation.evaluator]: \u001b[0mInference done 67/1474. 6.4022 s / img. ETA=2:30:09\n\u001b[32m[06/13 11:16:15 d2.evaluation.evaluator]: \u001b[0mInference done 68/1474. 6.4222 s / img. ETA=2:30:31\n\u001b[32m[06/13 11:16:23 d2.evaluation.evaluator]: \u001b[0mInference done 69/1474. 6.4515 s / img. ETA=2:31:06\n\u001b[32m[06/13 11:16:31 d2.evaluation.evaluator]: \u001b[0mInference done 70/1474. 6.4707 s / img. ETA=2:31:26\n\u001b[32m[06/13 11:16:38 d2.evaluation.evaluator]: \u001b[0mInference done 71/1474. 6.4775 s / img. ETA=2:31:29\n\u001b[32m[06/13 11:16:46 d2.evaluation.evaluator]: \u001b[0mInference done 72/1474. 6.5042 s / img. ETA=2:32:00\n\u001b[32m[06/13 11:16:56 d2.evaluation.evaluator]: \u001b[0mInference done 73/1474. 6.5568 s / img. ETA=2:33:08\n\u001b[32m[06/13 11:17:02 d2.evaluation.evaluator]: \u001b[0mInference done 74/1474. 6.5353 s / img. ETA=2:32:31\n\u001b[32m[06/13 11:17:08 d2.evaluation.evaluator]: \u001b[0mInference done 75/1474. 6.5277 s / img. ETA=2:32:14\n\u001b[32m[06/13 11:17:14 d2.evaluation.evaluator]: \u001b[0mInference done 76/1474. 6.5319 s / img. ETA=2:32:13\n\u001b[32m[06/13 11:17:24 d2.evaluation.evaluator]: \u001b[0mInference done 78/1474. 6.4850 s / img. ETA=2:30:54\n\u001b[32m[06/13 11:17:32 d2.evaluation.evaluator]: \u001b[0mInference done 79/1474. 6.5068 s / img. ETA=2:31:18\n\u001b[32m[06/13 11:17:42 d2.evaluation.evaluator]: \u001b[0mInference done 80/1474. 6.5503 s / img. ETA=2:32:13\n\u001b[32m[06/13 11:17:51 d2.evaluation.evaluator]: \u001b[0mInference done 81/1474. 6.5803 s / img. ETA=2:32:48\n\u001b[32m[06/13 11:17:57 d2.evaluation.evaluator]: \u001b[0mInference done 82/1474. 6.5819 s / img. ETA=2:32:43\n\u001b[32m[06/13 11:18:09 d2.evaluation.evaluator]: \u001b[0mInference done 84/1474. 6.5623 s / img. ETA=2:32:03\n\u001b[32m[06/13 11:18:15 d2.evaluation.evaluator]: \u001b[0mInference done 85/1474. 6.5551 s / img. ETA=2:31:47\n\u001b[32m[06/13 11:18:21 d2.evaluation.evaluator]: \u001b[0mInference done 86/1474. 6.5523 s / img. ETA=2:31:36\n\u001b[32m[06/13 11:18:35 d2.evaluation.evaluator]: \u001b[0mInference done 88/1474. 6.5562 s / img. ETA=2:31:28\n\u001b[32m[06/13 11:18:41 d2.evaluation.evaluator]: \u001b[0mInference done 89/1474. 6.5480 s / img. ETA=2:31:10\n\u001b[32m[06/13 11:18:47 d2.evaluation.evaluator]: \u001b[0mInference done 90/1474. 6.5399 s / img. ETA=2:30:53\n\u001b[32m[06/13 11:18:52 d2.evaluation.evaluator]: \u001b[0mInference done 91/1474. 6.5295 s / img. ETA=2:30:32\n\u001b[32m[06/13 11:18:59 d2.evaluation.evaluator]: \u001b[0mInference done 92/1474. 6.5297 s / img. ETA=2:30:26\n\u001b[32m[06/13 11:19:05 d2.evaluation.evaluator]: \u001b[0mInference done 93/1474. 6.5296 s / img. ETA=2:30:19\n\u001b[32m[06/13 11:19:12 d2.evaluation.evaluator]: \u001b[0mInference done 94/1474. 6.5333 s / img. ETA=2:30:17\n\u001b[32m[06/13 11:19:18 d2.evaluation.evaluator]: \u001b[0mInference done 95/1474. 6.5252 s / img. ETA=2:30:00\n\u001b[32m[06/13 11:19:26 d2.evaluation.evaluator]: \u001b[0mInference done 96/1474. 6.5387 s / img. ETA=2:30:12\n\u001b[32m[06/13 11:19:32 d2.evaluation.evaluator]: \u001b[0mInference done 97/1474. 6.5347 s / img. ETA=2:30:00\n\u001b[32m[06/13 11:19:38 d2.evaluation.evaluator]: \u001b[0mInference done 98/1474. 6.5284 s / img. ETA=2:29:45\n\u001b[32m[06/13 11:19:43 d2.evaluation.evaluator]: \u001b[0mInference done 99/1474. 6.5196 s / img. ETA=2:29:26\n\u001b[32m[06/13 11:19:53 d2.evaluation.evaluator]: \u001b[0mInference done 100/1474. 6.5499 s / img. ETA=2:30:01\n\u001b[32m[06/13 11:19:59 d2.evaluation.evaluator]: \u001b[0mInference done 101/1474. 6.5482 s / img. ETA=2:29:52\n\u001b[32m[06/13 11:20:04 d2.evaluation.evaluator]: \u001b[0mInference done 102/1474. 6.5343 s / img. ETA=2:29:27\n\u001b[32m[06/13 11:20:12 d2.evaluation.evaluator]: \u001b[0mInference done 103/1474. 6.5419 s / img. ETA=2:29:30\n\u001b[32m[06/13 11:20:18 d2.evaluation.evaluator]: \u001b[0mInference done 104/1474. 6.5440 s / img. ETA=2:29:27\n\u001b[32m[06/13 11:20:26 d2.evaluation.evaluator]: \u001b[0mInference done 105/1474. 6.5507 s / img. ETA=2:29:29\n\u001b[32m[06/13 11:20:33 d2.evaluation.evaluator]: \u001b[0mInference done 106/1474. 6.5543 s / img. ETA=2:29:28\n\u001b[32m[06/13 11:20:39 d2.evaluation.evaluator]: \u001b[0mInference done 107/1474. 6.5511 s / img. ETA=2:29:17\n\u001b[32m[06/13 11:20:47 d2.evaluation.evaluator]: \u001b[0mInference done 108/1474. 6.5650 s / img. ETA=2:29:29\n\u001b[32m[06/13 11:20:53 d2.evaluation.evaluator]: \u001b[0mInference done 109/1474. 6.5568 s / img. ETA=2:29:11\n\u001b[32m[06/13 11:20:58 d2.evaluation.evaluator]: \u001b[0mInference done 110/1474. 6.5507 s / img. ETA=2:28:57\n\u001b[32m[06/13 11:21:10 d2.evaluation.evaluator]: \u001b[0mInference done 112/1474. 6.5385 s / img. ETA=2:28:27\n\u001b[32m[06/13 11:21:16 d2.evaluation.evaluator]: \u001b[0mInference done 113/1474. 6.5333 s / img. ETA=2:28:13\n\u001b[32m[06/13 11:21:23 d2.evaluation.evaluator]: \u001b[0mInference done 114/1474. 6.5381 s / img. ETA=2:28:13\n\u001b[32m[06/13 11:21:30 d2.evaluation.evaluator]: \u001b[0mInference done 115/1474. 6.5400 s / img. ETA=2:28:09\n\u001b[32m[06/13 11:21:41 d2.evaluation.evaluator]: \u001b[0mInference done 117/1474. 6.5251 s / img. ETA=2:27:36\n\u001b[32m[06/13 11:21:51 d2.evaluation.evaluator]: \u001b[0mInference done 119/1474. 6.4970 s / img. ETA=2:26:45\n\u001b[32m[06/13 11:21:58 d2.evaluation.evaluator]: \u001b[0mInference done 120/1474. 6.5000 s / img. ETA=2:26:42\n\u001b[32m[06/13 11:22:10 d2.evaluation.evaluator]: \u001b[0mInference done 122/1474. 6.4867 s / img. ETA=2:26:11\n\u001b[32m[06/13 11:22:21 d2.evaluation.evaluator]: \u001b[0mInference done 123/1474. 6.5250 s / img. ETA=2:26:57\n\u001b[32m[06/13 11:22:28 d2.evaluation.evaluator]: \u001b[0mInference done 124/1474. 6.5329 s / img. ETA=2:27:01\n\u001b[32m[06/13 11:22:34 d2.evaluation.evaluator]: \u001b[0mInference done 125/1474. 6.5303 s / img. ETA=2:26:51\n\u001b[32m[06/13 11:22:40 d2.evaluation.evaluator]: \u001b[0mInference done 126/1474. 6.5227 s / img. ETA=2:26:34\n\u001b[32m[06/13 11:22:48 d2.evaluation.evaluator]: \u001b[0mInference done 127/1474. 6.5393 s / img. ETA=2:26:50\n\u001b[32m[06/13 11:22:54 d2.evaluation.evaluator]: \u001b[0mInference done 128/1474. 6.5329 s / img. ETA=2:26:35\n\u001b[32m[06/13 11:23:02 d2.evaluation.evaluator]: \u001b[0mInference done 129/1474. 6.5394 s / img. ETA=2:26:37\n\u001b[32m[06/13 11:23:12 d2.evaluation.evaluator]: \u001b[0mInference done 130/1474. 6.5665 s / img. ETA=2:27:07\n\u001b[32m[06/13 11:23:17 d2.evaluation.evaluator]: \u001b[0mInference done 131/1474. 6.5593 s / img. ETA=2:26:51\n\u001b[32m[06/13 11:23:29 d2.evaluation.evaluator]: \u001b[0mInference done 133/1474. 6.5498 s / img. ETA=2:26:25\n\u001b[32m[06/13 11:23:36 d2.evaluation.evaluator]: \u001b[0mInference done 134/1474. 6.5505 s / img. ETA=2:26:19\n\u001b[32m[06/13 11:23:42 d2.evaluation.evaluator]: \u001b[0mInference done 135/1474. 6.5514 s / img. ETA=2:26:14\n\u001b[32m[06/13 11:23:48 d2.evaluation.evaluator]: \u001b[0mInference done 136/1474. 6.5449 s / img. ETA=2:25:59\n\u001b[32m[06/13 11:23:55 d2.evaluation.evaluator]: \u001b[0mInference done 137/1474. 6.5442 s / img. ETA=2:25:51\n\u001b[32m[06/13 11:24:01 d2.evaluation.evaluator]: \u001b[0mInference done 138/1474. 6.5436 s / img. ETA=2:25:44\n\u001b[32m[06/13 11:24:11 d2.evaluation.evaluator]: \u001b[0mInference done 139/1474. 6.5660 s / img. ETA=2:26:07\n\u001b[32m[06/13 11:24:21 d2.evaluation.evaluator]: \u001b[0mInference done 140/1474. 6.5929 s / img. ETA=2:26:36\n\u001b[32m[06/13 11:24:29 d2.evaluation.evaluator]: \u001b[0mInference done 141/1474. 6.6071 s / img. ETA=2:26:49\n\u001b[32m[06/13 11:24:35 d2.evaluation.evaluator]: \u001b[0mInference done 142/1474. 6.6039 s / img. ETA=2:26:38\n\u001b[32m[06/13 11:24:41 d2.evaluation.evaluator]: \u001b[0mInference done 143/1474. 6.5963 s / img. ETA=2:26:21\n^C\nTraceback (most recent call last):\n  File \"train_net.py\", line 159, in <module>\n    args=(args,),\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 57, in launch\n    main_func(*args)\n  File \"train_net.py\", line 116, in main\n    res = Trainer.test(cfg, model)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 519, in test\n    results_i = inference_on_dataset(model, data_loader, evaluator)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/evaluation/evaluator.py\", line 141, in inference_on_dataset\n    outputs = model(inputs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/modeling/meta_arch/rcnn.py\", line 106, in forward\n    return self.inference(batched_inputs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/modeling/meta_arch/rcnn.py\", line 154, in inference\n    features = self.backbone(images.tensor)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/modeling/backbone/fpn.py\", line 123, in forward\n    bottom_up_features = self.bottom_up(x)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/modeling/backbone/resnet.py\", line 427, in forward\n    x = stage(x)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n    input = module(input)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/modeling/backbone/resnet.py\", line 198, in forward\n    out = self.conv2(out)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/detectron2/layers/wrappers.py\", line 94, in forward\n    x = super().forward(x)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 349, in forward\n    return self._conv_forward(input, self.weight)\n  File \"/Users/angyizhe/opt/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 346, in _conv_forward\n    self.padding, self.dilation, self.groups)\nKeyboardInterrupt\n"
    }
   ],
   "source": [
    "!python train_net.py --eval-only --config-file {str(config_path)} --exp-name cascade_lr MODEL.WEIGHTS {weights_path} OUTPUT_DIR output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data')/'til_2020'\n",
    "eval_coco_path = data_dir/'CV_interim_evaluation.json'\n",
    "val_coco_path = data_dir/'val_clean.json'\n",
    "img_dir = data_dir/'CV_interim_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with eval_coco_path.open('r') as read_file:\n",
    "    eval_coco = json.load(read_file)\n",
    "\n",
    "with val_coco_path.open('r') as read_file:\n",
    "    val_coco = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['images', 'categories'])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "eval_coco.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'name': 'tops', 'id': 1},\n {'name': 'trousers', 'id': 2},\n {'name': 'outerwear', 'id': 3},\n {'name': 'dresses', 'id': 4},\n {'name': 'skirts', 'id': 5}]"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "eval_coco['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'id': 1, 'name': 'tops'},\n {'id': 2, 'name': 'trousers'},\n {'id': 3, 'name': 'outerwear'},\n {'id': 4, 'name': 'dresses'},\n {'id': 5, 'name': 'skirts'}]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "val_coco['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'file_name': '6964.jpg', 'id': 6964}"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "eval_coco['images'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(config_path)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.WEIGHTS = str(weights_path)\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use original image size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "\n",
    "# For each image\n",
    "for img_dict in eval_coco['images']:\n",
    "    file_name = img_dict['file_name']\n",
    "    img_id = img_dict['id']\n",
    "\n",
    "    img = cv2.imread(str(img_dir/file_name))\n",
    "\n",
    "    outputs = predictor(img)\n",
    "    instances = outputs['instances']\n",
    "    pred_classes = np.array(instances.pred_classes)\n",
    "    pred_boxes = instances.pred_boxes\n",
    "    scores = np.array(instances.scores)\n",
    "\n",
    "    # For each bounding box\n",
    "    for i, box in enumerate(instances.pred_boxes):\n",
    "        box = np.array(box)\n",
    "        x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        category_id = pred_classes[i] + 1\n",
    "        score = scores[i]\n",
    "\n",
    "        sub_dict = {\n",
    "            'image_id': img_id,\n",
    "            'category_id': category_id,\n",
    "            'bbox': [x1, y1, width, height],\n",
    "            'score': score\n",
    "        }\n",
    "\n",
    "        submissions.append(sub_dict)\n",
    "    \n",
    "    print(f'Image {img_id} done...')\n",
    "\n",
    "print('Inference complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = eval_coco['images'][0]['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(str(img_dir/file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor(img)\n",
    "instances = outputs['instances']\n",
    "pred_classes = instances.pred_classes\n",
    "pred_boxes = instances.pred_boxes\n",
    "scores = instances.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9711182117462158"
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "scores[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_coco_path = 'data/til_2020/CV_final_evaluation.json'\n",
    "eval_img_dir = Path('data')/'til_2020'/'CV_final_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_coco_path, 'r') as f:\n",
    "    eval_coco = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['images', 'categories', 'annotations'])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "eval_coco.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'id': 10044, 'file_name': '10044.jpg', 'height': 2965, 'width': 1977}"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "eval_coco['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'name': 'tops', 'id': 1},\n {'name': 'trousers', 'id': 2},\n {'name': 'outerwear', 'id': 3},\n {'name': 'dresses', 'id': 4},\n {'name': 'skirts', 'id': 5}]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "eval_coco['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Height and Width for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'id': 10044, 'file_name': '10044.jpg'}"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "eval_coco['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1972"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(eval_coco['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_dict in eval_coco['images']:\n",
    "    file_name = img_dict['file_name']\n",
    "    img = Image.open(eval_img_dir/file_name)\n",
    "    img_dict['height'] = img.height\n",
    "    img_dict['width'] = img.width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_coco['annotations'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_coco_path, 'w') as f:\n",
    "    json.dump(eval_coco, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}